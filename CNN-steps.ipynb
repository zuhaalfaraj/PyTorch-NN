{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking the size of data through CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, Im trying to build a Convolutional Neural Network(CNN) step by step, and track the size and no. of channels changes after passing each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "data_dir = 'PetImages'\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(32),\n",
    "                                transforms.CenterCrop(32),\n",
    "                                transforms.ToTensor()])\n",
    "dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, ax=None, title=None, normalize=True):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    if normalize:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "    ax.imshow(image)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.tick_params(axis='both', length=0)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f3d5f268c88>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3d5f342898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(images[0], normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Size is(batches, num of channels, hight, width): torch.Size([32, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print('Image Size is(batches, num of channels, hight, width): {}'.format(images.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- One Conv. Layer\n",
    "\n",
    "With only one conv. layer, the hight and width of output will be the same, but the number of channels will be changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diagrams/1.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN1(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN1, self).__init__()\n",
    "        # (depth of input, desired output , kernel size)\n",
    "        self.conv1 = nn.Conv2d(3 , 16 , 3, padding=1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))        \n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model = CNN1()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The convolutional output when we have only one conv-layer is(batches, channels, hight, width) : torch.Size([32, 16, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "Output1 = model.forward(images)\n",
    "print('The convolutional output when we have only one conv-layer is(batches, channels, hight, width) : {}'\n",
    "      .format(Output1.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Adding MaxPool\n",
    "\n",
    "Maxpool layer will divide the hight and width by its value \n",
    "\n",
    "so if we have 2x2 maxpool ---> high/2 , width/2\n",
    "\n",
    "3x3 ---> high/3 , width/3,  and so on.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diagrams/2.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN2(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN2, self).__init__()\n",
    "        # (depth of input, desired output , kernel size)\n",
    "        self.conv1 = nn.Conv2d(3 , 16 , 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  \n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model2 = CNN2()\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The convolutional output after adding maxpool(batches, channels, hight, width) : torch.Size([32, 16, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "Output2 = model2.forward(images)\n",
    "print('The convolutional output after adding maxpool(batches, channels, hight, width) : {}'\n",
    "      .format(Output2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3- Adding another conv-layer + maxpool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diagrams/3.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN3(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN3, self).__init__()\n",
    "        #(depth of input, desired output , kernel size)\n",
    "        self.conv1 = nn.Conv2d(3 , 16 , 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16 , 32 , 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  \n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))  \n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model3 = CNN3()\n",
    "print(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The convolutional output with two conv-layers and maxpools(batches, channels, hight, width) : torch.Size([32, 32, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "Output3 = model3.forward(images)\n",
    "print('The convolutional output with two conv-layers and maxpools(batches, channels, hight, width) : {}'\n",
    "      .format(Output3.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Adding a third Conv. Layer + Maxpool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diagrams/4.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN4(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN4, self).__init__()\n",
    "        #(depth of input, desired output , kernel size)\n",
    "        self.conv1 = nn.Conv2d(3 , 16 , 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16 , 32 , 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32 , 64 , 3, padding=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  \n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))  \n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))  \n",
    "        x = self.pool(x)\n",
    "\n",
    "        \n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model4 = CNN4()\n",
    "print(model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The convolutional output after adding third layer(batches, channels, hight, width) : torch.Size([32, 64, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "Output4= model4.forward(images)\n",
    "print('The convolutional output after adding third layer(batches, channels, hight, width) : {}'\n",
    "      .format(Output4.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5- Adding a Fully-connected Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diagrams/5.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN5(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=1024, out_features=500, bias=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN5, self).__init__()\n",
    "        #(depth of input, desired output , kernel size)\n",
    "        self.conv1 = nn.Conv2d(3 , 16 , 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16 , 32 , 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32 , 64 , 3, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(4*4*64, 500)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  \n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))  \n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))  \n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "\n",
    "        x= self.fc1(x)\n",
    "\n",
    "        \n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model5 = CNN5()\n",
    "print(model5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After adding a fully connected layer, the information of all previous layers are dedicated on one 1D layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The convolutional output after adding fully connected layer(batches, width) : torch.Size([32, 500])\n"
     ]
    }
   ],
   "source": [
    "Output5= model5.forward(images)\n",
    "print('The convolutional output after adding fully connected layer(batches, width) : {}'\n",
    "      .format(Output5.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6- Adding an Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diagrams/6.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN6(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=1024, out_features=500, bias=True)\n",
      "  (output): Linear(in_features=500, out_features=2, bias=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN6(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN6, self).__init__()\n",
    "        #(depth of input, desired output , kernel size)\n",
    "        self.conv1 = nn.Conv2d(3 , 16 , 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16 , 32 , 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32 , 64 , 3, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(4*4*64, 500)\n",
    "        \n",
    "        self.output = nn.Linear(500, 2)\n",
    "\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  \n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))  \n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))  \n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "        x= F.relu(self.fc1(x))\n",
    "        x =self.output(x)\n",
    "\n",
    "        \n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model6 = CNN6()\n",
    "print(model6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The convolutional output after adding fully connected layer(batches, width) : torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "Output6= model6.forward(images)\n",
    "print('The convolutional output after adding fully connected layer(batches, width) : {}'\n",
    "      .format(Output6.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
